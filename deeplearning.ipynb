{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LASI 2022\n",
    "\n",
    "## Deep Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Big Picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/bigpicture.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bigpicture1.png\" style=\"width: 75%; height: 75%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Neuron**: the *atomic computational units* of deep learning networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bigpicture2.png\" style=\"width: 75%; height: 75%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Layers**: neurons are organized in stacked layers to achieve increasingly abstract data representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bigpicture3.png\" style=\"width: 75%; height: 75%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Forward Propagation**: the end-to-end computational process for generating predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bigpicture4.png\" style=\"width: 75%; height: 75%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Loss and Cost Function**: method for quantifying the error or discrepancy between predictions and ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bigpicture5.png\" style=\"width: 75%; height: 75%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Backward Propagation**: the computational process for systematically reducing the error by adjusting the network's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/bigpicture.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# I. Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/function.png\" style=\"width: 50%; height: 50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Artificial Neurons are derived from biological neurons but can be viewed as mathematical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A neuron accepts a set of inputs, performs a computation on that input, and returns an output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neuron's Two Step Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/neuron2steps.png\" style=\"width: 50%; height: 50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Neurons perform their computation in two-steps. The function in Step 1 is called the *transfer* function. Its output is sent to the *activation* function in Step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1: Neuron Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/transferfunction.png\" style=\"width: 75%; height: 75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each neuron accepts $n$ inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A \"weight\" is associated with each input: $x_{n} \\mapsto w_{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Then a \"bias\" term $b$ is added to the weighted sum of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 1:  Computation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/transferexample.png\" style=\"width: 75%; height: 75%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step 2: Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/activationfunction1.png\" style=\"width: 75%; height: 75%\" >|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/activationfunction2.png\" style=\"width: 75%; height: 75%\">|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/activationfunction3.png\" style=\"width: 75%; height: 75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Step2: Computation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/activationexample.png\" style=\"width: 75%; height: 75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We represent the input $\\mathbf{x}$ as a column vector. $\\mathbf{x} =\\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots \\\\\n",
    "    x_n\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We represent the weights $\\mathbf{w}$ as a column vector but take it's transform to obtain a row vector $\\mathbf{w}^\\top = \\begin{bmatrix}\n",
    "    w_1\\\n",
    "    w_2\\\n",
    "    \\dots \\\n",
    "    w_n\n",
    "    \\end{bmatrix}$. We represent the bias $\\mathbf{b}$ as a scalar $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The result of the transfer function computation: $z = \\begin{bmatrix}\n",
    "    w_1\\\n",
    "    w_2\\\n",
    "    \\dots \\\n",
    "    w_n\n",
    "    \\end{bmatrix} \\bullet \\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots \\\\\n",
    "    x_n\n",
    "\\end{bmatrix} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The result of the transfer function computation:  $z = \\mathbf{w}^\\top \\mathbf{x} + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The result of the activation function computation: $a$ = $\\phi(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neuron Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "def heaviside(z):\n",
    "    if z<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \n",
    "    # isize = size or number of inputs\n",
    "    def __init__(self,size):\n",
    "        self.weights = np.random.randn(size)\n",
    "        self.bias = np.random.randn()\n",
    "        \n",
    "    # computes z = weighted sum + bias\n",
    "    def y_hat(self,x,phi):\n",
    "        z = np.dot(self.weights,x) + self.bias\n",
    "        a = phi(z)\n",
    "        print(a)\n",
    "        return(a)\n",
    "    \n",
    "    # show parameters\n",
    "    def show_params(self):\n",
    "        print(\"weights: =\", self.weights)\n",
    "        print(\"biases: = \", self.bias)\n",
    "    \n",
    "    \n",
    "    # updates weights and biases\n",
    "    def update_params(self,weights,bias):\n",
    "        self.weights = np.array([weights])\n",
    "        self.bias = np.array([bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: = [-0.54017605  0.26314472  1.04401152]\n",
      "biases: =  -0.9199287111683621\n",
      "0.5936397556747696\n",
      "weights: = [[ 3.2 -1.9  2.5]]\n",
      "biases: =  [-1]\n",
      "[0.74269055]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.74269055])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2.3,4.5,1.3]\n",
    "weights = [3.2,-1.9,2.5]\n",
    "bias = -1\n",
    "\n",
    "N1 = Neuron(3)\n",
    "N1.show_params()\n",
    "N1.y_hat(x,sigmoid)\n",
    "\n",
    "\n",
    "N1.update_params(weights,bias)\n",
    "N1.show_params()\n",
    "N1.y_hat(x,sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: = [-0.87047531  0.71872396  1.47511814 -0.0582848   0.02974996]\n",
      "biases: =  0.637140682984003\n"
     ]
    }
   ],
   "source": [
    "N1 = Neuron(5)\n",
    "N1.show_params()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.3,4.5,1.3,2.1,-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N1.y_hat(x,heaviside)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [3.2,-1.9,2.5,3.3,-2]\n",
    "bias = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9711343080237546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9711343080237546"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N1.y_hat(x,sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/layers1.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A neural network consists of multiple neurons organized in layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/dense.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A densely connected network means that each neuron in each layer is connected to all neurons in the previous and subsequent layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/layers2.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A deep learning network consists of three types of layers: an input layer, one or more hidden layers, and an output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The *size* of the network is the number of hidden layers plus the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Parameters and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The *weights* and *biases* of the neural network are its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A deep learning model *learns* the correct weights and biases in the process of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Hyperparameters are structural characteristics of the network, including the number of layers, the number of neurons in each layer, the activation functions, the particular loss function, the optimization function, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/feedforward1.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Feedforward networks are the quintessential neural network. They are referred to as feedforward because information flows sequentially through each layer without feedback or loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Forward Propagation Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/feedforward2.png\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A feedforward neural network composes together a series of functions: $f(x) = f^{(3)}(f^{(2)}(f^{(1)}(x)))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A deep learning model *learns* the correct weights and biases in the process of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "    \n",
    "def heaviside(z):\n",
    "    if z<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    def __init__(self,sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) \n",
    "                        for x,y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self,a,phi):\n",
    "        for b,w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w,a)+b\n",
    "            a = phi(z)\n",
    "        return a\n",
    "        \n",
    "    def set_biases(self,newbiases):\n",
    "        self.biases = [np.array(newbiases)]\n",
    "        \n",
    "    def set_weights(self,newweights):\n",
    "        self.weights = [np.array([newweights])]\n",
    "        \n",
    "    def show_parameters(self):\n",
    "        print(\"biases: =\", self.biases)\n",
    "        print(\"weights: =\", self.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biases: = [array([[1.61556786]]), array([[-0.21402557],\n",
      "       [ 0.20582364],\n",
      "       [-2.59502659]])]\n",
      "weights: = [array([[-1.97269426, -1.17618158, -0.50006713, -1.13947644]]), array([[ 1.80230017],\n",
      "       [-0.05597854],\n",
      "       [ 0.11217766]])]\n"
     ]
    }
   ],
   "source": [
    "deep = Network([4,1,3])\n",
    "deep.show_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.446752  ],\n",
       "       [0.55127331],\n",
       "       [0.06946008]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep.feedforward([2,3,4,1],sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " $$ Z = \\begin{bmatrix} { w }_{ 11 } & { w}_{ 12 } & {w}_{13}\\\\ { w }_{ 21 } & { w }_{ 22 } &{w}_{23} \\\\ \\vdots & \\vdots & \\vdots \\\\ { w }_{ n1 } & { w }_{ n2 }& {w}_{n3} \\end{bmatrix} \\bullet \\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    \\vdots \\\\\n",
    "    x_n\n",
    "\\end{bmatrix} + \\begin{bmatrix}\n",
    "    b_1\\\\\n",
    "    b_2\\\\\n",
    "    \\vdots \\\\\n",
    "    b_n\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ a = \\phi (\\mathbf{W}^\\top \\cdot \\mathbf{X} + \\mathbf{B}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# IV. Loss and Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation: Why do we need Cost Function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/target.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Imagine we are shooting arrows during target practice or a competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We need a way to measure how far off target we are with each arrow (\"loss\") and with all arrows (\"cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We could use a simple function, namely distance from center to express the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## And the Challenger is Katniss Everdeen!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=images\\lawrence.jpg style=\"float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\">\n",
    "<img src=images\\contest.png style=\"float: left; width: 30%; margin-right: 1%; margin-bottom: 0.5em;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The loss function computes the error for a single training example. The cost function is the average of the loss functions of the entire training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Mean Squared Error** (MSE) is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Cross Entropy** measures the performance of a classification model whose output is a probability value between 0 and 1. $$\\frac 1n \\sum_{i=1}^ny_i\\log \\hat{y}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# V. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Given an artificial neural network and an cost function, the method calculates the gradient of the cost function with respect to the neural network's weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/what.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/costfunction.png\" width=\"75%\" height=\"75%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Our goal is to reduce error (\"minimize the cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The error is a function of the parameters of our network. We are looking for the optimum set of weights (i.e. cost is 0 or is as small as possible.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Where I am is my current set of weights. Where I want to be is the set of weights where error or cost is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What does Calculus have to do with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- By calculating derivative we can find minimum and maximum of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Minimums and maximums are found where the first derivative (slope) is zero. By taking a second derivative we can determine whether it's a minimum or maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f(x) = x^2$$\n",
    "\n",
    "$$f'(x) = 2x$$\n",
    "\n",
    "$$ 2x = 0  $$ \n",
    "\n",
    "$$f''x =2$$\n",
    "\n",
    "$$f''>0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Not out of the Woods!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- In deep neural networks we have cost functions with hundreds of thousands of parameters!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We can't get at minimums analytically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- We have to calculate gradients and move incrementally reach minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stochastic Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/contour.jpg\" width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Assumes we can't find minimum analytically\n",
    "- We calculate gradient at current point. Negative gradient tells us *direction* of steepest descent.\n",
    "- We take incremental step towards steepest descent. \n",
    "- We recalculate error based on new coordinates (i.e. new weights or parameters)\n",
    "- We iterate until we converge to minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Stochastic Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bigpicture.png\"  />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "214px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 1,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
